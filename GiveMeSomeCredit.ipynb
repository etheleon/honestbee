{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Challenge : Give Me Some Credit\n",
    "\n",
    "\n",
    "<img src=\"https://kaggle2.blob.core.windows.net/competitions/kaggle/2551/logos/front_page.png\" style=\"width:200px;height:100px;\">\n",
    "\n",
    "Give Me Some Credit\n",
    "Improve on the state of the art in credit scoring by predicting the probability that somebody will experience financial distress in the next two years.\n",
    "\n",
    "Banks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit. \n",
    "\n",
    "Credit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. This competition requires participants to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years.\n",
    "\n",
    "The goal of this competition is to build a model that borrowers can use to help make the best financial decisions.\n",
    "\n",
    "Historical data are provided on 250,000 borrowers and \n",
    "the prize pool is 5,000 \n",
    "\n",
    "* 3,000 for first, \n",
    "* 1,500 for second and \n",
    "* 500 for third.\n",
    "\n",
    "## Methods\n",
    "\n",
    "I'm going to implement a ensemble classifier using a couple of methods:\n",
    "\n",
    "* xgboost\n",
    "* Random Forest \n",
    "* SVM\n",
    "* PCA\n",
    "* Artificial Neural Networks\n",
    "\n",
    "## Scoring Metric: Area Under the Curve (AUC)\n",
    "\n",
    "Evaluation is done us the AUC or Receiver operating characteristic (ROC), sometimes also referred collectively as \n",
    " Area Under the Receiver Operating Characteristic curve.\n",
    " \n",
    "\n",
    "Y-axis is True Positive Rate (TPR) / Sensitivity\n",
    "X-axis is the False Positive Rate (FPR) / 1 - specificity\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Sensitivity_and_specificity.svg/700px-Sensitivity_and_specificity.svg.png\"  style=\"height:400px; width:200px\">\n",
    "<caption><center> **ROC AUC**</center></caption><br>\n",
    "\n",
    "TPR = True positives / Positives \n",
    "Positives = True Positives + False Negativs\n",
    "\n",
    "FPR = False Positives / Negatives\n",
    "Negatives = True Negatives + False Positives\n",
    "\n",
    "<img src=\"http://mchp-appserv.cpe.umanitoba.ca/concept/roc_gif_small.gif\">\n",
    "<caption><center> **Figure 1**</center></caption><br>\n",
    "\n",
    "Thus having a high AUC curve will be to have a high sensitivity while keeping the false positive rate low\n",
    "\n",
    "Besides AUC/ROC there's also log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets import the universe\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import rpy2\n",
    "%load_ext rpy2.ipython\n",
    "# use R's ggplot to plot instead\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "#machine learning\n",
    "## xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "## sklearn\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion     #\n",
    "from sklearn_pandas import DataFrameMapper    #\n",
    "from sklearn_pandas import CategoricalImputer #\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer\n",
    "#score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import subprocess\n",
    "%connect_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data\n",
    "\n",
    "Below's the description of the input data:\n",
    "\n",
    "| Variable Name | Description | Type |\n",
    "| --- | --- | --- |\n",
    "| SeriousDlqin2yrs  | Person experienced 90 days past due delinquency or worse |  Y/N |\n",
    "| RevolvingUtilizationOfUnsecuredLines  | Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits  | percentage | \n",
    "| age | Age of borrower in years  | integer |\n",
    "| NumberOfTime30-59DaysPastDueNotWorse  | Number of times borrower has been 30-59 days past due but no worse in the last 2 years. | integer |\n",
    "| DebtRatio | Monthly debt payments, alimony,living costs divided by monthy gross income | percentage |\n",
    "| MonthlyIncome | Monthly income  | real |\n",
    "| NumberOfOpenCreditLinesAndLoans | Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)  | integer |\n",
    "| NumberOfTimes90DaysLate | Number of times borrower has been 90 days or more past due. | integer |\n",
    "| NumberRealEstateLoansOrLines  | Number of mortgage and real estate loans including home equity lines of credit  | integer| \n",
    "| NumberOfTime60-89DaysPastDueNotWorse  | Number of times borrower has been 60-89 days past due but no worse in the last 2 years. | integer | \n",
    "| NumberOfDependents |  Number of dependents in family excluding themselves (spouse, children etc.) | integer | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "\n",
    "## 1. Class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, testDF = loadData(preprocessed=False)\n",
    "defaulted = np.sum(y_train != 0) + np.sum(y_test != 0)\n",
    "clean = np.sum(y_train == 0) + np.sum(y_test == 0)\n",
    "\n",
    "classSep = pd.DataFrame({\n",
    "    \"class\":[\"defaulted\",\"clean\"],\n",
    "    \"value\":[defaulted, clean]\n",
    "}, index=[0,1])\n",
    "classSep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array(classSep['value']/np.sum(classSep['value']))\n",
    "classweight = {1:weights[0], 0:weights[1]}\n",
    "classweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i classSep -w 10 -h 5 -u in\n",
    "\n",
    "suppressPackageStartupMessages({\n",
    "    library(tidyverse)\n",
    "    library(cowplot)\n",
    "})\n",
    "\n",
    "suppressMessages({\n",
    "    p1 = ggplot(classSep, aes(class, value, fill=class))+ \n",
    "        geom_histogram(stat=\"identity\") +\n",
    "        geom_text(aes(y=value, label=value)) + \n",
    "        scale_fill_discrete(\"Default\", labels=c(\"Good\", \"Default\")) +\n",
    "        scale_y_log10() + ggtitle(\"Y-Log10 scale\")\n",
    "     p2 = ggplot(classSep, aes(class, value, fill=class))+ \n",
    "        geom_histogram(stat=\"identity\") +\n",
    "        geom_text(aes(y=value, label=value)) + \n",
    "        scale_fill_discrete(\"Default\", labels=c(\"Good\", \"Default\")) + ggtitle(\"Linear Y Scale\")\n",
    "    p = plot_grid(p1, p2, nrow=1) \n",
    "        })   \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -i X_train \n",
    "\n",
    "suppressWarnings({ \n",
    "        library(tidyverse)\n",
    "        library(GGally)\n",
    "        ggpairs(X_train) %>% ggsave(filename=\"ggpairs.png\", w=30, h=30, dpi=300) \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ggpairs.png\">\n",
    "<caption><center> **Figure 2**: Pairs plot</center></caption><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, testDF = loadData(logTransform=True, preprocessed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R -i X_train \n",
    "\n",
    "suppressWarnings({ \n",
    "        library(tidyverse)\n",
    "        library(GGally)\n",
    "        X_train %>% select(-DebtRatio, -NumberOfTime30.59DaysPastDueNotWorse, -MonthlyIncome, \n",
    "                                    -NumberOfOpenCreditLinesAndLoans, -NumberOfTimes90DaysLate, \n",
    "                                    -NumberRealEstateLoansOrLines, -NumberOfTime60.89DaysPastDueNotWorse) %>% \n",
    "        ggpairs() %>%\n",
    "        ggsave(filename=\"ggpairslog.png\", w=30, h=30, dpi=300) \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./ggpairslog.png\">\n",
    "<caption><center> **Figure 2**: Pairs plot</center></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model2: XGB\n",
    "\n",
    "Now that we know the logistic regression's error rate lets try a few more models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with log columns\n",
    "X_train, X_test, y_train, y_test, testDF = loadData(logTransform=True, \n",
    "                                                    impute=False, \n",
    "                                                    preprocessed=False)\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "np.sum(y_train)/len(y_train), np.sum(y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standardised sklearn pipeline with XGB\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "# Dont really need cause none of the columns are objects but lets just keep it \n",
    "categorical_feature_mask = X_train.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X_train.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X_train.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer (using median/mean) both gives almost the same value\n",
    "# aka fill the NaNs\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "   [([numeric_feature], Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "   input_df=True,\n",
    "   df_out=True\n",
    ")\n",
    "\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "    [(category_feature, Categorical()) for category_feature in categorical_columns],\n",
    "    input_df=True,\n",
    "    df_out=True\n",
    ")\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "    (\"num_mapper\", numeric_imputation_mapper),\n",
    "    (\"cat_mapper\", categorical_imputation_mapper)\n",
    "])\n",
    "\n",
    "#param['tree_method'] = 'gpu_hist'\n",
    "\n",
    "params = { \n",
    "        \"n_estimators\": 400, \n",
    "        'tree_method':['gpu_hist'], \n",
    "        'predictor':['gpu_predictor'] \n",
    "         }\n",
    "\n",
    "# Create full pipeline\n",
    "#pipeline = Pipeline([\n",
    "#   (\"featureunion\", numeric_imputation_mapper),\n",
    "#   (\"clf\", xgb.XGBClassifier(max_depth=3, scale_pos_weight=1)) #class imbalance\n",
    "#])\n",
    "weights = (y_train == 0).sum() / (1.0 * (y_train == 1).sum())\n",
    "pipeline = Pipeline([\n",
    "   (\"featureunion\", numeric_imputation_mapper),\n",
    "   (\"clf\", xgb.XGBClassifier(max_depth=10, \n",
    "                         scale_pos_weight=weights, \n",
    "                             gamma=20\n",
    "                            )) #class imbalance\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "#cross_val_scores_cpu = cross_val_score(pipeline, X_train, y_train, scoring=\"roc_auc\", cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = pipeline.fit(X_train, y_train)\n",
    "dev = model.predict(X_test)\n",
    "roc_auc_score(dev, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsSubmit = model.predict(testDF)\n",
    "submit(preds, \"xgb_straitified_processing_weights_gamma.csv\", \"xgb stratified preprocessing weights gamma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Standardised sklearn pipeline with XGB\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "# Dont really need cause none of the columns are objects but lets just keep it \n",
    "categorical_feature_mask = X_train.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X_train.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X_train.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer (using median/mean) both gives almost the same value\n",
    "# aka fill the NaNs\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "   [([numeric_feature], Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "   input_df=True,\n",
    "   df_out=True\n",
    ")\n",
    "\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "    [(category_feature, Categorical()) for category_feature in categorical_columns],\n",
    "    input_df=True,\n",
    "    df_out=True\n",
    ")\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "    (\"num_mapper\", numeric_imputation_mapper),\n",
    "    (\"cat_mapper\", categorical_imputation_mapper)\n",
    "])\n",
    "\n",
    "#tried running with GPU (nope doesnt work)\n",
    "#param['tree_method'] = 'gpu_hist'\n",
    "params = { \n",
    "        \"n_estimators\": 400, \n",
    "        'tree_method':['gpu_hist'], \n",
    "        'predictor':['gpu_predictor'] \n",
    "         }\n",
    "\n",
    "# Create full pipeline\n",
    "#pipeline = Pipeline([\n",
    "#   (\"featureunion\", numeric_imputation_mapper),\n",
    "#   (\"clf\", xgb.XGBClassifier(max_depth=3, scale_pos_weight=1)) #class imbalance\n",
    "#])\n",
    "weights = (y_train == 0).sum() / (1.0 * (y_train == 1).sum())\n",
    "pipeline = Pipeline([\n",
    "   (\"featureunion\", numeric_imputation_mapper),\n",
    "   (\"clf\", xgb.XGBClassifier(max_depth=10, \n",
    "                         scale_pos_weight=weights, \n",
    "                             gamma=20\n",
    "                            )) #class imbalance\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "#cross_val_scores_cpu = cross_val_score(pipeline, X_train, y_train, scoring=\"roc_auc\", cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = pipeline.fit(X_train, y_train)\n",
    "dev = model.predict(X_test)\n",
    "roc_auc_score(dev, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsSubmit = model.predict(testDF)\n",
    "submit(preds, \"xgb_straitified_processing_weights_gamma.csv\", \"xgb stratified preprocessing weights gamma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing!!!\n",
    "\n",
    "Adding the weights, and the gamma set to damn high 20, my public and private rose\n",
    "\n",
    "0.775060, 0.769128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning, we could do either one of two \n",
    "\n",
    "1. Randomised Search\n",
    "2. Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_param_grid = {\n",
    "        'clf__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "            'clf__max_depth': np.arange(3, 10, 1),\n",
    "        'clf__n_estimators': np.arange(50, 200, 50)#,\n",
    "        #'clf__gamma':[5,10,13,16,19,20]\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "grid_roc_auc = GridSearchCV(pipeline,\n",
    "    param_grid=gbm_param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the estimator\n",
    "grid_roc_auc.fit(X_train, y_train)\n",
    "\n",
    "# Compute metrics\n",
    "print(f'my best score: {grid_roc_auc.best_score_}')\n",
    "print(grid_roc_auc.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = grid_roc_auc.predict(X_test)\n",
    "accuracy = float(np.sum(preds==y_test)/y_test.shape[0] )\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'AUC: {roc_auc_score(preds, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting with the gamma turned on the AUC is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = grid_roc_auc.predict(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predsSubmit = model.predict(testDF)\n",
    "submit(preds, \"xgb_straitified_processing_weights_gamma_gridsearch2.csv\", \"xgb stratified preprocessing weights gamma grid2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_roc_auc.best_estimator_.named_steps['clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the grid search\n",
    "\n",
    "| public | private |  \n",
    "| --- | --- | \n",
    "| 0.790252 0.783193 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "   (\"featureunion\", numeric_imputation_mapper),\n",
    "   (\"rf\",RandomForestClassifier(random_state=123, n_jobs=-1, class_weight=classweight, n_estimators=600))\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(rf_pipeline, X_train, y_train, scoring=\"roc_auc\", cv=10)\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "    \n",
    "dev = model.predict(X_test)\n",
    "testScore = roc_auc_score(dev, y_test)\n",
    "    \n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))\n",
    "print(\"test AUC: \", testScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPreds = model.predict(testDF)\n",
    "#nestimators 500 or 400 gets the same score\n",
    "submit(rfPreds, \"rf_500.csv\", \"rf 500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF is doing quite similarly with XGB, the private, public LB is: 0.775060, 0.769128 compared with XGB's 0.790252, 0.783193 but with lesser tunning for hyperparameters required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking / ensembling\n",
    "\n",
    "\n",
    "tried stacking with a metaclassifier (linear regression) doesnt work its, worse. \n",
    "tried ensembling (vote) better but the score is still worse than individual predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test, testDF = loadData(\n",
    "    logTransform=True, \n",
    "    impute=False, \n",
    "    preprocessed=False, \n",
    "    continuous=False\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe1 = make_pipeline(numeric_imputation_mapper,\n",
    "                      XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=1, gamma=20, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=3, min_child_weight=1, missing=None, n_estimators=150,\n",
    "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=13.960728088766986,\n",
    "       seed=None, silent=True, subsample=1))\n",
    "\n",
    "pipe2 = make_pipeline(numeric_imputation_mapper,\n",
    "                      RandomForestClassifier(max_depth=None, random_state=123, class_weight=classweight, n_estimators=400)\n",
    "                      )\n",
    "\n",
    "\n",
    "# Initializing models\n",
    "#clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "#using the gridSearch version\n",
    "#clf2 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#       colsample_bytree=1, gamma=20, learning_rate=0.1, max_delta_step=0,\n",
    "#       max_depth=3, min_child_weight=1, missing=None, n_estimators=150,\n",
    "#       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
    "#       reg_alpha=0, reg_lambda=1, scale_pos_weight=13.960728088766986,\n",
    "#       seed=None, silent=True, subsample=1)\n",
    "#clf1 = RandomForestClassifier(max_depth=None, random_state=123, class_weight=classweight, n_estimators=500)\n",
    "#clf2 = XGBClassifier(max_depth=3, scale_pos_weight=1) #class imbalance\n",
    "#clf3 = GaussianNB()\n",
    "#clf3 = LogisticRegression()\n",
    "eclf = EnsembleVoteClassifier(clfs=[pipe1, pipe2],\n",
    "#eclf = EnsembleVoteClassifier(clfs=[clf1, clf2],\n",
    "                              weights=[1, 1], voting='soft')\n",
    "#sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "#                          meta_classifier=lr)\n",
    "\n",
    "#params = {\n",
    "#    'logisticregression__C':  [0.001, 0.01]\n",
    "#}\n",
    "\n",
    "#grid = GridSearchCV(estimator=sclf, \n",
    "#grid = GridSearchCV(estimator=eclf, \n",
    "#                    param_grid=params, \n",
    "#                    cv=5,\n",
    "#                    n_jobs=-1,\n",
    "#                    scoring='roc_auc',\n",
    "#                    refit=True)\n",
    "#X_train.head()\n",
    "#newX = numeric_imputation_mapper.transform(X_train)\n",
    "#grid.fit(X_train, y_train)\n",
    "eclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "#for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "#    print(\"%0.3f +/- %0.2f %r\"\n",
    "#          % (grid.cv_results_[cv_keys[0]][r],\n",
    "#             grid.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "#             grid.cv_results_[cv_keys[2]][r]))\n",
    "#newXtest = numeric_imputation_mapper.transform(X_test)\n",
    "#devstack = grid.predict(newXtest)\n",
    "#testScore = roc_auc_score(devstack, y_test)\n",
    "#newXtest = numeric_imputation_mapper.transform(X_test)\n",
    "#devstack = grid.predict(X_test)\n",
    "devstack = eclf.predict(X_test)\n",
    "testScore = roc_auc_score(devstack, y_test)\n",
    "#print('Best parameters: %s' % grid.best_params_)\n",
    "print(f'test AUC: {testScore}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newXtest = numeric_imputation_mapper.transform(testDF)\n",
    "#stackPreds = grid.predict(newXtest)\n",
    "votePreds = eclf.predict(testDF)\n",
    "submit(votePreds, \"xgb_rf_voting2.csv\", \"xgb rf voting2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding RF and XGB brought the score down: private, public: \n",
    "        0.721574, 0.722799\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multilayer perceptron\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "#layers\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#import tensorflow as tf\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X_train_matrix = X_train.as_matrix(columns=[X_train.columns[:]])\n",
    "X_train_matrix[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=16, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=2000, verbose=1)\n",
    "kfold = StratifiedKFold(y_train.values, n_folds=5, shuffle=True, random_state=123)\n",
    "results = cross_val_score(estimator, X_train_matrix, y_train.values, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(X_train_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorLayer = estimator.predict(X_test_matrix)\n",
    "accuracy = float(np.sum(estimatorLayer.flatten() == y_test)/y_test.shape[0])\n",
    "np.unique(estimatorLayer.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Severely overfitting, time to do some feature engineering and ensembling\n",
    "\n",
    "# Multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "#def create_model():\n",
    "    # create model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(16,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(256, activation='relu', activity_regularizer=regularizers.l1(10e-5)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "#model.add(Dense(32, activation='relu', activity_regularizer=regularizers.l1(10e-5)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid')) #output\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "nb_epoch = 100\n",
    "batch_size = 2000\n",
    "\n",
    "#def train_and_evaluate__model(model, data, label, data_test, label_test):\n",
    "\n",
    "model.fit(\n",
    "        X_train_matrix, y_train.values, validation_split=0.1, \n",
    "        #callbacks=[early_stopping],\n",
    "        class_weight=[0.5, 100], \n",
    "        epochs = nb_epoch,\n",
    "        batch_size = batch_size\n",
    ")\n",
    "# skf = StratifiedKFold(y_train, n_folds=10, shuffle=True)\n",
    "# for i, (train, test) in enumerate(skf):\n",
    "#     print \"Running Fold\", i+1, \"/\", n_folds\n",
    "#     model = None # Clearing the NN.\n",
    "#     model = create_model()\n",
    "#     train_and_evaluate_model(model, X_train[train], y_train[train], X_test[test], y_test[test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_matrix.shape, np.sum(y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_matrix = X_test.as_matrix(columns=[X_test.columns[:]])\n",
    "preds = model.predict(X_train_matrix)\n",
    "preds\n",
    "#fail the neural net still thinks everyone is 0\n",
    "#accuracy = float(np.sum(preds.flatten() == y_test)/y_test.shape[0])\n",
    "#accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder\n",
    "\n",
    "(didnt work the loss is incredulous)\n",
    "this is based from the inspiration i got from this post by Veneline Valkov\n",
    "\n",
    "https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, testDF = loadData(logTransform=False, \n",
    "                                                    impute=False, \n",
    "                                                    preprocessed=True, continuous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i X_train \n",
    "\n",
    "suppressWarnings({ \n",
    "        library(tidyverse)\n",
    "        library(GGally)\n",
    "        X_train %>% #select(-DebtRatio, -NumberOfTime30.59DaysPastDueNotWorse, -MonthlyIncome, \n",
    "#                                    -NumberOfOpenCreditLinesAndLoans, -NumberOfTimes90DaysLate, \n",
    "#                                    -NumberRealEstateLoansOrLines, -NumberOfTime60.89DaysPastDueNotWorse) %>% \n",
    "        ggpairs() %>%\n",
    "        ggsave(filename=\"ggpairs_selected.png\", w=30, h=30, dpi=300) \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./ggpairs_selected.png\">\n",
    "<caption><center> **Figure 3**: Pairs plot</center></caption><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "defaulting = X_train[y_train == 1]\n",
    "normal = X_train[y_train == 0]\n",
    "normal_test = X_test[y_test == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = normal.values\n",
    "normal_test = normal_test.values\n",
    "normal_test.shape, normal.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 18 \n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "autoencoder = Model(input=input_layer, output=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "nb_epoch = 100\n",
    "batch_size = 1000\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "autoencoder.summary()\n",
    "checkpointer = ModelCheckpoint(filepath=\"model2.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(normal, normal,\n",
    "                    nb_epoch=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(normal_test, normal_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[tensorboard, checkpointer]).history\n",
    "\n",
    "#gave up the loss is too fucking big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
